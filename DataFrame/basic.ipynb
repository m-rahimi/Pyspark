{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dataframes').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('walmart_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType\n",
    "\n",
    "# data_schema = [StructField(\"age\", IntegerType(), True), # True means accept null data\n",
    "#                StructField(\"name\", StringType(), True)]\n",
    "\n",
    "# final_struc = StructType(fields=data_schema)\n",
    "\n",
    "# df = spark.read.csv('walmart_stock.csv',header=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2012, 1, 3, 0, 0), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date=datetime.datetime(2012, 1, 4, 0, 0), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2) # show two first lines as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark create row object and column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"Open\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select a column or create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|         59.970001|\n",
      "|60.209998999999996|\n",
      "|         59.349998|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"Open\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|               new|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|        119.940002|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|120.41999799999999|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|        118.699996|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new\", df[\"Open\"]*2).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|          new_Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"Open\", \"new_Open\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-20 00:00:00|             60.75|    61.25|60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write a single SQL string\n",
    "df.filter(\"Open > 60 and Open < 80\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-20 00:00:00|             60.75|    61.25|60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To pass multiple conditions to filter or where use Column objects and logical operators (&, |, ~)\n",
    "from pyspark.sql.functions import col\n",
    "df.filter((col(\"Open\") > 60) & (col(\"Open\") < 100)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-20 00:00:00|             60.75|    61.25|60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Open\"] > 60) & (df[\"Open\"] < 100)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|              Open|\n",
      "+-------------------+------------------+\n",
      "|2012-01-04 00:00:00|60.209998999999996|\n",
      "|2012-01-20 00:00:00|             60.75|\n",
      "+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Open\"] > 60) & (df[\"Open\"] < 100)).select([\"Date\", \"Open\"]).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the data from a specific line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2012, 1, 20, 0, 0), Open=60.75, High=61.25, Low=60.669998, Close=61.009997999999996, Volume=10378800, Adj Close=53.212320999999996),\n",
       " Row(Date=datetime.datetime(2012, 1, 24, 0, 0), Open=60.75, High=62.0, Low=60.75, Close=61.389998999999996, Volume=7362800, Adj Close=53.54375400000001)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = df.filter(df[\"Open\"] == 60.75).collect()\n",
    "print(type(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2012, 1, 20, 0, 0),\n",
       " 'Open': 60.75,\n",
       " 'High': 61.25,\n",
       " 'Low': 60.669998,\n",
       " 'Close': 61.009997999999996,\n",
       " 'Volume': 10378800,\n",
       " 'Adj Close': 53.212320999999996}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = result[0]\n",
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|Open_int|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|      59|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|      60|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|      59|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "df.withColumn(\"Open_int\", df[\"Open\"].cast(IntegerType())).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|Open_int|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|      59|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|      60|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|      59|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def float_to_int(x):\n",
    "    return int(x)\n",
    "\n",
    "udffloat_to_int = udf(float_to_int, IntegerType())\n",
    "\n",
    "df.withColumn(\"Open_int\", udffloat_to_int(\"Open\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add new column using function or new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+---+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close| x4|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+---+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|  0|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|  0|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"x4\", lit(0)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_int = udf(lambda z: square(z), IntegerType())\n",
    "square_udf_float = udf(lambda z: square(z), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+---------+\n",
      "|              Open|SQR_int|SQR_float|\n",
      "+------------------+-------+---------+\n",
      "|         59.970001|   3481|3596.4011|\n",
      "|60.209998999999996|   3600| 3625.244|\n",
      "|         59.349998|   3481|3522.4224|\n",
      "+------------------+-------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Open\", square_udf_int(df[\"Open\"].cast(IntegerType())).alias(\"SQR_int\"), square_udf_float(\"Open\").alias(\"SQR_float\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(x):\n",
    "    if x < 50 : return \"A\"\n",
    "    elif x < 60 : return \"B\"\n",
    "    elif x < 70 : return \"C\"\n",
    "    elif x < 80 : return \"D\"\n",
    "    else : return \"E\"\n",
    "    \n",
    "udf_group = udf(lambda z: group(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = df.select(\"Open\", \"Close\", udf_group(\"Open\").alias(\"Group\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Group|count|\n",
      "+-----+-----+\n",
      "|    E|  112|\n",
      "|    B|   82|\n",
      "|    D|  748|\n",
      "|    C|  316|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da.groupby(\"Group\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|Group|         avg(Open)|        avg(Close)|\n",
      "+-----+------------------+------------------+\n",
      "|    E| 84.31178542857143|  84.3377680982143|\n",
      "|    B|58.917926902439035|58.973170731707306|\n",
      "|    D|  74.8132084759358| 74.82470592379677|\n",
      "|    C| 65.79655097468358| 65.86759499050635|\n",
      "+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da.groupby(\"Group\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+\n",
      "|Group|min(Close)|max(Open)|\n",
      "+-----+----------+---------+\n",
      "|    E| 79.739998|90.800003|\n",
      "|    B| 56.419998|59.970001|\n",
      "|    D| 68.940002|79.839996|\n",
      "|    C|     58.98|69.980003|\n",
      "+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da.groupby(\"Group\").agg({\"Open\":\"max\", \"Close\":\"min\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|Group|      max|      min|\n",
      "+-----+---------+---------+\n",
      "|    E|90.800003|79.739998|\n",
      "|    B|59.970001|56.419998|\n",
      "|    D|79.839996|68.940002|\n",
      "|    C|69.980003|    58.98|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min, mean\n",
    "\n",
    "da.groupby(\"Group\").agg(max(da[\"Open\"]).alias(\"max\"), \n",
    "                        min(da[\"Close\"]).alias(\"min\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|Group|    Close|     Open|\n",
      "+-----+---------+---------+\n",
      "|    E|79.739998|90.800003|\n",
      "|    B|56.419998|59.970001|\n",
      "|    D|68.940002|79.839996|\n",
      "|    C|    58.98|69.980003|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "columns_and_operations = {\"Open\":\"max\", \"Close\":\"min\"}\n",
    "\n",
    "dg = da.groupby(\"Group\").agg(columns_and_operations)\n",
    "\n",
    "old_names = [\"{}({})\".format(v, k) for k, v in columns_and_operations.items()]\n",
    "new_names = list(columns_and_operations.keys())\n",
    "\n",
    "reduce(lambda dg, i: dg.withColumnRenamed(old_names[i], new_names[i]), range(len(old_names)), dg).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get max, min, or any statistic from a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(Open)=90.800003)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.agg(max(df.Open)).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.800003"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(da.agg(max(df.Open)).collect()[0].asDict().values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              Avg|\n",
      "+-----------------+\n",
      "|72.35785375357709|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "da.select(avg('Open').alias('Avg')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.35785375357709"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(da.select(avg('Open').alias('Avg')).collect()[0].asDict().values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'72.358'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number \n",
    "list(da.select(avg('Open').alias('Avg')).select(format_number('Avg', 3)).collect()[0].asDict().values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2015-11-16 00:00:00|56.389998999999996|         58.029999|56.360001000000004|         57.869999|13321600|55.362759999999994|\n",
      "|2015-11-13 00:00:00|56.740002000000004|         57.060001|         56.299999|         56.419998|12514900|53.975581000000005|\n",
      "|2015-11-02 00:00:00|57.290001000000004|57.610001000000004|             56.77|57.610001000000004|10719200|         55.114026|\n",
      "|2015-11-03 00:00:00|             57.57|         58.330002|         57.529999|58.110001000000004|10253900|         55.592364|\n",
      "|2012-04-26 00:00:00|             57.59|             59.43|             57.57|         58.950001|25092900|         51.759753|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('Open').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+---------+---------+--------+-----------------+\n",
      "|               Date|             Open|             High|      Low|    Close|  Volume|        Adj Close|\n",
      "+-------------------+-----------------+-----------------+---------+---------+--------+-----------------+\n",
      "|2015-01-13 00:00:00|        90.800003|        90.970001|    88.93|89.309998| 8215400|        83.825448|\n",
      "|2015-01-09 00:00:00|            90.32|        90.389999|    89.25|89.349998| 8522500|        83.862993|\n",
      "|2015-01-12 00:00:00|        89.360001|        90.309998|89.220001|90.019997| 7372500|        84.491846|\n",
      "|2015-01-08 00:00:00|        89.209999|90.66999799999999|    89.07|90.470001|12713600|84.91421600000001|\n",
      "|2015-01-23 00:00:00|88.41999799999999|        89.260002|87.889999|88.510002| 7565800|83.07458100000001|\n",
      "+-------------------+-----------------+-----------------+---------+---------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['Open'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     name| id|\n",
      "+---------+---+\n",
      "|   Pirate|  1|\n",
      "|   Monkey|  2|\n",
      "|    Ninja|  3|\n",
      "|Spaghetti|  4|\n",
      "+---------+---+\n",
      "\n",
      "+-----------+---+\n",
      "|       name| id|\n",
      "+-----------+---+\n",
      "|   Rutabaga|  1|\n",
      "|     Pirate|  2|\n",
      "|      Ninja|  3|\n",
      "|Darth Vader|  4|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valuesA = [('Pirate',1),('Monkey',2),('Ninja',3),('Spaghetti',4)]\n",
    "TableA = spark.createDataFrame(valuesA,['name','id'])\n",
    " \n",
    "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','id'])\n",
    " \n",
    "TableA.show()\n",
    "TableB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = TableA.alias('ta')\n",
    "tb = TableB.alias('tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+\n",
      "|  name| id|  name| id|\n",
      "+------+---+------+---+\n",
      "| Ninja|  3| Ninja|  3|\n",
      "|Pirate|  1|Pirate|  2|\n",
      "+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "ta.join(tb, ta.name == tb.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----+\n",
      "|     name| id|  name|  id|\n",
      "+---------+---+------+----+\n",
      "|Spaghetti|  4|  null|null|\n",
      "|    Ninja|  3| Ninja|   3|\n",
      "|   Pirate|  1|Pirate|   2|\n",
      "|   Monkey|  2|  null|null|\n",
      "+---------+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta.join(tb, ta.name == tb.name, how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name| id| new|\n",
      "+---------+---+----+\n",
      "|Spaghetti|  4|null|\n",
      "|    Ninja|  3|   3|\n",
      "|   Pirate|  1|   2|\n",
      "|   Monkey|  2|null|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ta.join(tb, ta.name == tb.name, how=\"left\").select(ta.name, ta.id, tb.id.alias(\"new\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------+----+\n",
      "|     name|  id|      name2| id2|\n",
      "+---------+----+-----------+----+\n",
      "|     null|null|   Rutabaga|   1|\n",
      "|Spaghetti|   4|       null|null|\n",
      "|    Ninja|   3|      Ninja|   3|\n",
      "|   Pirate|   1|     Pirate|   2|\n",
      "|   Monkey|   2|       null|null|\n",
      "|     null|null|Darth Vader|   4|\n",
      "+---------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn = ta.join(tb, ta.name == tb.name, how='full').select(ta.name, ta.id, tb.name.alias(\"name2\"), tb.id.alias(\"id2\"))\n",
    "dn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### work with null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+\n",
      "|  name| id| name2|id2|\n",
      "+------+---+------+---+\n",
      "| Ninja|  3| Ninja|  3|\n",
      "|Pirate|  1|Pirate|  2|\n",
      "+------+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------+----+\n",
      "|     name|  id|      name2| id2|\n",
      "+---------+----+-----------+----+\n",
      "|     null|null|   Rutabaga|   1|\n",
      "|Spaghetti|   4|       null|null|\n",
      "|    Ninja|   3|      Ninja|   3|\n",
      "|   Pirate|   1|     Pirate|   2|\n",
      "|   Monkey|   2|       null|null|\n",
      "|     null|null|Darth Vader|   4|\n",
      "+---------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------+----+\n",
      "|     name|  id|      name2| id2|\n",
      "+---------+----+-----------+----+\n",
      "|     null|null|   Rutabaga|   1|\n",
      "|Spaghetti|   4|       null|null|\n",
      "|    Ninja|   3|      Ninja|   3|\n",
      "|   Pirate|   1|     Pirate|   2|\n",
      "|   Monkey|   2|       null|null|\n",
      "|     null|null|Darth Vader|   4|\n",
      "+---------+----+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop(how='any').show() # how='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---+\n",
      "|  name|  id|      name2|id2|\n",
      "+------+----+-----------+---+\n",
      "|  null|null|   Rutabaga|  1|\n",
      "| Ninja|   3|      Ninja|  3|\n",
      "|Pirate|   1|     Pirate|  2|\n",
      "|  null|null|Darth Vader|  4|\n",
      "+------+----+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop(subset=[\"name2\", \"id2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+----+\n",
      "|     name| id|      name2| id2|\n",
      "+---------+---+-----------+----+\n",
      "|     null|  2|   Rutabaga|   1|\n",
      "|Spaghetti|  4|       null|null|\n",
      "|    Ninja|  3|      Ninja|   3|\n",
      "|   Pirate|  1|     Pirate|   2|\n",
      "|   Monkey|  2|       null|null|\n",
      "|     null|  2|Darth Vader|   4|\n",
      "+---------+---+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_value = dn.select(mean(dn[\"id\"])).collect()[0][0]\n",
    "dn.na.fill(subset=\"id\", value=mean_value).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name| id|name2|id2|\n",
      "+----+---+-----+---+\n",
      "|   2|  2|    2|  2|\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "dn.select([count(when(col(c).isNull(), c)).alias(c) for c in dn.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name| id|name2|id2|\n",
      "+----+---+-----+---+\n",
      "|   2|  2|    2|  2|\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dn.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'count(CASE WHEN (isnan(name) OR (name IS NULL)) THEN name END) AS `name`'>,\n",
       " Column<b'count(CASE WHEN (isnan(id) OR (id IS NULL)) THEN id END) AS `id`'>,\n",
       " Column<b'count(CASE WHEN (isnan(name2) OR (name2 IS NULL)) THEN name2 END) AS `name2`'>,\n",
       " Column<b'count(CASE WHEN (isnan(id2) OR (id2 IS NULL)) THEN id2 END) AS `id2`'>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dn.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|(name IS NULL)|\n",
      "+--------------+\n",
      "|          true|\n",
      "|         false|\n",
      "|         false|\n",
      "|         false|\n",
      "|         false|\n",
      "|          true|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.select(dn[\"name\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### median and qurtile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68.620003, 73.230003, 76.629997]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Name of the column\n",
    "#List of values signifying which quantile we want. In our example we are calculating 25%, 50% and 75%. 50% is same as median\n",
    "#The last parameter signifies the error rate. 0.0 signifies we want exact value.\n",
    "df.stat.approxQuantile(\"Open\", [0.25, 0.5, 0.75],0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.filter(df[\"Open\"] > 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258 858\n"
     ]
    }
   ],
   "source": [
    "print(df.count(), df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.union(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2116"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop dublicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop_duplicates(subset=[\"Open\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofmonth, dayofyear, hour, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1.258000e+03</td>\n",
       "      <td>1258.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>72.357854</td>\n",
       "      <td>72.839388</td>\n",
       "      <td>71.918601</td>\n",
       "      <td>72.388450</td>\n",
       "      <td>8.222093e+06</td>\n",
       "      <td>67.238838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.768090</td>\n",
       "      <td>6.768187</td>\n",
       "      <td>6.744076</td>\n",
       "      <td>6.756859</td>\n",
       "      <td>4.519781e+06</td>\n",
       "      <td>6.722609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>56.389999</td>\n",
       "      <td>57.060001</td>\n",
       "      <td>56.299999</td>\n",
       "      <td>56.419998</td>\n",
       "      <td>2.094900e+06</td>\n",
       "      <td>50.363689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>68.627503</td>\n",
       "      <td>69.059998</td>\n",
       "      <td>68.162502</td>\n",
       "      <td>68.632497</td>\n",
       "      <td>5.791100e+06</td>\n",
       "      <td>63.778335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.235000</td>\n",
       "      <td>73.725002</td>\n",
       "      <td>72.839996</td>\n",
       "      <td>73.265000</td>\n",
       "      <td>7.093500e+06</td>\n",
       "      <td>68.541162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>76.629997</td>\n",
       "      <td>77.094999</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>76.709999</td>\n",
       "      <td>9.394675e+06</td>\n",
       "      <td>71.105668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.800003</td>\n",
       "      <td>90.970001</td>\n",
       "      <td>89.250000</td>\n",
       "      <td>90.470001</td>\n",
       "      <td>8.089810e+07</td>\n",
       "      <td>84.914216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close        Volume  \\\n",
       "count  1258.000000  1258.000000  1258.000000  1258.000000  1.258000e+03   \n",
       "mean     72.357854    72.839388    71.918601    72.388450  8.222093e+06   \n",
       "std       6.768090     6.768187     6.744076     6.756859  4.519781e+06   \n",
       "min      56.389999    57.060001    56.299999    56.419998  2.094900e+06   \n",
       "25%      68.627503    69.059998    68.162502    68.632497  5.791100e+06   \n",
       "50%      73.235000    73.725002    72.839996    73.265000  7.093500e+06   \n",
       "75%      76.629997    77.094999    76.250000    76.709999  9.394675e+06   \n",
       "max      90.800003    90.970001    89.250000    90.470001  8.089810e+07   \n",
       "\n",
       "         Adj Close  \n",
       "count  1258.000000  \n",
       "mean     67.238838  \n",
       "std       6.722609  \n",
       "min      50.363689  \n",
       "25%      63.778335  \n",
       "50%      68.541162  \n",
       "75%      71.105668  \n",
       "max      84.914216  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         min(Open)|\n",
      "+------------------+\n",
      "|56.389998999999996|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(df[\"Open\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
